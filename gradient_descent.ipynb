{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get data in np array\n",
    "data = pd.read_csv(\"data/train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change format to np\n",
    "data = np.array(data)\n",
    "\n",
    "# Get shape of data and calculate where to split test set\n",
    "m, n = data.shape\n",
    "split = (int)(0.2 * m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42000"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Randomize data order\n",
    "np.random.shuffle(data)\n",
    "\n",
    "# Get test data\n",
    "data_test = data[0:split].T\n",
    "y_test = data_test[0]\n",
    "X_test = data_test[1:n]\n",
    "X_test = X_test / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get train data\n",
    "data_train = data[split:m].T\n",
    "y_train = data_train[0]\n",
    "X_train = data_train[1:n]\n",
    "X_train = X_train / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params():\n",
    "    '''\n",
    "    Randomizes weights (W) and biases (b) for initialization\n",
    "\n",
    "    Returns W1, b1, W2, b2\n",
    "    '''\n",
    "\n",
    "    # Randomized values from -0.5 to 0.5 in matrices for weights and biases\n",
    "    # Layer 1\n",
    "    W1 = np.random.rand(10, 784) - 0.5\n",
    "    b1 = np.random.rand(10, 1) - 0.5\n",
    "\n",
    "    # Layer 2\n",
    "    W2 = np.random.rand(10, 10) - 0.5\n",
    "    b2 = np.random.rand(10, 1) - 0.5\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n",
    "def ReLU(Z):\n",
    "    '''\n",
    "    Applies ReLU function to unactivated layer (Z)\n",
    "\n",
    "    Returns A\n",
    "    '''\n",
    "\n",
    "    # By returning max of current value or 0, makes all negatives 0 aka ReLU function\n",
    "    A = np.maximum(0, Z)\n",
    "\n",
    "    return A\n",
    "\n",
    "def softmax(Z):\n",
    "    '''\n",
    "    Applies softmax function to unactivated layer (Z)\n",
    "\n",
    "    Returns A\n",
    "    '''\n",
    "\n",
    "    # Returns e to the power of each value in the current layer divided by the sum of all e to the powers in current layer aka softmax\n",
    "    A = np.exp(Z) / sum(np.exp(Z))\n",
    "\n",
    "    return A\n",
    "\n",
    "def forward_prop(W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, X: np.ndarray):\n",
    "    '''\n",
    "    Creates unactivated layers (Z) from input, weights, and biases\n",
    "        then activates those layers with activation functions, turning them into A\n",
    "\n",
    "    Returns Z1, A1, Z2, A2\n",
    "    '''\n",
    "\n",
    "    # Creates Z1 layer from W1, X, and b1 and activates with ReLU\n",
    "    Z1 = W1.dot(X) + b1\n",
    "    A1 = ReLU(Z1)\n",
    "\n",
    "    # Creates Z2 layer from W2, A1, and b2 and activates with softmax\n",
    "    Z2 = W2.dot(A1) + b2\n",
    "    A2 = softmax(Z2)\n",
    "\n",
    "    return Z1, A1, Z2, A2\n",
    "\n",
    "def one_hot(Y: np.ndarray, n_classes: int):\n",
    "    '''\n",
    "    Makes an array with a new array of size equal to number of classes with the nth value as \n",
    "        one to indicate class and all others as 0 for each n in Y\n",
    "\n",
    "    Returns one_hot_Y\n",
    "    '''\n",
    "\n",
    "    # Creates number of arrays equal to size of Y made up of amount of 0s equal to n_classes\n",
    "    one_hot_Y = np.zeros((Y.size, n_classes))\n",
    "\n",
    "    # For each array, make the index, given by current value in 1, 0\n",
    "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
    "    \n",
    "    # Transpose matrix\n",
    "    one_hot_Y = one_hot_Y.T\n",
    "\n",
    "    return one_hot_Y\n",
    "\n",
    "def deriv_ReLU(Z: np.ndarray):\n",
    "    '''\n",
    "    Applies the derivative of the ReLU function by replacing value with 1 if greater than 0,\n",
    "        as slope of ReLU is 1 for 0 and beyond, and 0 if less than 0, as the slope of ReLU\n",
    "        is 0 for less than 0\n",
    "    \n",
    "    Returns g\n",
    "    '''\n",
    "\n",
    "    g = Z > 0\n",
    "\n",
    "    return g\n",
    "\n",
    "def back_prop(Z1: np.ndarray, A1: np.ndarray, Z2: np.ndarray, A2: np.ndarray, W2: np.ndarray, X: np.ndarray, Y: np.ndarray, n_classes: int):\n",
    "    '''\n",
    "    Gets the difference from the predicted and actual values and essentially performs steps in\n",
    "        reverse to find out how much the original weights were off\n",
    "\n",
    "    Returns dW1, db1, dW2, db2 \n",
    "    '''\n",
    "\n",
    "    # Get one hot encoded Y values\n",
    "    one_hot_Y = one_hot(Y, n_classes)\n",
    "\n",
    "    # Calculate difference of Z2 by getting difference between activated layer 2 and actual Ys\n",
    "    dZ2: np.ndarray = 1 / m * (A2 - one_hot_Y)\n",
    "\n",
    "    # Calculate difference of W2 by calculating 1/m times the dot-product of the differences of Z2 and the tranpose of A1\n",
    "    dW2: np.ndarray = dZ2.dot(A1.T)\n",
    "\n",
    "    # Calculate difference of b2 by calculating 1/m times the sum of the values of difference of Z2\n",
    "    db2: np.ndarray = np.sum(dZ2)\n",
    "\n",
    "    # Calculate difference of Z1 by the dot-product of the transpose of W2 and the differences of Z2 times the derivative of ReLU applied to Z1\n",
    "    dZ1: np.ndarray = W2.T.dot(dZ2) * deriv_ReLU(Z1)\n",
    "\n",
    "    # Calculate difference of W1 by calculating 1/m times the dot-product of the differences of Z1 and the tranpose of X\n",
    "    dW1: np.ndarray = dZ1.dot(X.T)\n",
    "\n",
    "    # Calculate difference of b1 by calculating 1/m times the sum of the values of difference of Z1\n",
    "    db1: np.ndarray = np.sum(dZ1)\n",
    "\n",
    "    return dW1, db1, dW2, db2\n",
    "\n",
    "def update_params(W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, dW1: np.ndarray, db1: np.ndarray, dW2: np.ndarray, db2: np.ndarray, alpha: float):\n",
    "    '''\n",
    "    Updates params by subtracting the calculated difference multiplied by \n",
    "        alpha, the learning rate\n",
    "\n",
    "    Returns W1, b1, W2, b2\n",
    "    '''\n",
    "    \n",
    "    #Update weights and biases\n",
    "    # Layer 1\n",
    "    W1 = W1 - alpha * dW1\n",
    "    b1 = b1 - alpha * db1\n",
    "\n",
    "    # Layer 2\n",
    "    W2 = W2 - alpha * dW2\n",
    "    b2 = b2 - alpha * db2\n",
    "\n",
    "    return W1, b1, W2, b2\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_preds(A2: np.ndarray):\n",
    "    '''\n",
    "    Gets predictions from last activated layer (A2) by getting the max value\n",
    "        for each array\n",
    "    \n",
    "    Returns preds\n",
    "    '''\n",
    "\n",
    "    preds = np.argmax(A2, 0)\n",
    "\n",
    "    return preds\n",
    "\n",
    "def get_accuracy(preds: np.ndarray, Y: np.ndarray):\n",
    "    '''\n",
    "    Gets accuracy of predictions vs real by summing up all the occurences of\n",
    "        predictions being equal to real and dividing by the size\n",
    "    \n",
    "    Returns accuracy_score\n",
    "    '''\n",
    "\n",
    "    accuracy_score = np.sum(preds == Y) / Y.size\n",
    "\n",
    "    return accuracy_score\n",
    "\n",
    "def get_accuracy_test(W1: np.ndarray, b1: np.ndarray, W2: np.ndarray, b2: np.ndarray, X: np.ndarray, Y: np.ndarray):\n",
    "    '''\n",
    "    Gets accuracy of predictions vs real by summing up all the occurences of\n",
    "        predictions being equal to real and dividing by the size\n",
    "    \n",
    "    Returns accuracy_score\n",
    "    '''\n",
    "\n",
    "    # Get last activated layer with forward_prop\n",
    "    Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "\n",
    "    # Get predictions from data in last activated layer\n",
    "    preds = get_preds(A2)\n",
    "\n",
    "    # Calculate accuracy score by summing up the ones that match the real and divide by size\n",
    "    accuracy_score = np.sum(preds == Y) / Y.size\n",
    "\n",
    "    return accuracy_score\n",
    "\n",
    "def gradient_descent(X: np.ndarray, Y: np.ndarray, iterations: int, alpha: float, n_classes: int, X_test_data: np.ndarray, Y_test_data: np.ndarray):\n",
    "    '''\n",
    "    Goes through learning process, printing out accuracy every \n",
    "    '''\n",
    "\n",
    "    # Get initial random weights\n",
    "    W1, b1, W2, b2 = init_params()\n",
    "\n",
    "    # Loop for each iteration\n",
    "    for i in range(iterations + 1):\n",
    "        \n",
    "        # Get layers with forward_prop\n",
    "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
    "\n",
    "        # Find error with back_prop\n",
    "        dW1, db1, dW2, db2 = back_prop(Z1, A1, Z2, A2, W2, X, Y, n_classes)\n",
    "\n",
    "        # Update params based off error and learning rate\n",
    "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
    "\n",
    "        # Every 50 iterations print out stats\n",
    "        if i % 50 == 0:\n",
    "            print(f\"Iteration: {i}\")\n",
    "            print(f\"Train Accuracy: {get_accuracy(get_preds(A2), Y)}\")\n",
    "            print(f\"Test Accuracy: {get_accuracy_test(W1, b1, W2, b2, X_test_data, Y_test_data)}\")\n",
    "            print()\n",
    "    \n",
    "    # Return final weights and biases of model\n",
    "    return W1, b1, W2, b2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0\n",
      "Train Accuracy: 0.10401785714285715\n",
      "Test Accuracy: 0.1767857142857143\n",
      "\n",
      "Iteration: 50\n",
      "Train Accuracy: 0.6432738095238095\n",
      "Test Accuracy: 0.6422619047619048\n",
      "\n",
      "Iteration: 100\n",
      "Train Accuracy: 0.7748214285714285\n",
      "Test Accuracy: 0.7783333333333333\n",
      "\n",
      "Iteration: 150\n",
      "Train Accuracy: 0.825\n",
      "Test Accuracy: 0.8228571428571428\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m W1, b1, W2, b2 \u001b[38;5;241m=\u001b[39m \u001b[43mgradient_descent\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 56\u001b[0m, in \u001b[0;36mgradient_descent\u001b[0;34m(X, Y, iterations, alpha, n_classes, X_test_data, Y_test_data)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Loop for each iteration\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     54\u001b[0m     \n\u001b[1;32m     55\u001b[0m     \u001b[38;5;66;03m# Get layers with forward_prop\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m     Z1, A1, Z2, A2 \u001b[38;5;241m=\u001b[39m \u001b[43mforward_prop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;66;03m# Find error with back_prop\u001b[39;00m\n\u001b[1;32m     59\u001b[0m     dW1, db1, dW2, db2 \u001b[38;5;241m=\u001b[39m back_prop(Z1, A1, Z2, A2, W2, X, Y, n_classes)\n",
      "Cell \u001b[0;32mIn[16], line 57\u001b[0m, in \u001b[0;36mforward_prop\u001b[0;34m(W1, b1, W2, b2, X)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Creates Z2 layer from W2, A1, and b2 and activates with softmax\u001b[39;00m\n\u001b[1;32m     56\u001b[0m Z2 \u001b[38;5;241m=\u001b[39m W2\u001b[38;5;241m.\u001b[39mdot(A1) \u001b[38;5;241m+\u001b[39m b2\n\u001b[0;32m---> 57\u001b[0m A2 \u001b[38;5;241m=\u001b[39m \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Z1, A1, Z2, A2\n",
      "Cell \u001b[0;32mIn[16], line 39\u001b[0m, in \u001b[0;36msoftmax\u001b[0;34m(Z)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03mApplies softmax function to unactivated layer (Z)\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \n\u001b[1;32m     35\u001b[0m \u001b[38;5;124;03mReturns A\u001b[39;00m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m'''\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Returns e to the power of each value in the current layer divided by the sum of all e to the powers in current layer aka softmax\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m A \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m \u001b[38;5;28msum\u001b[39m(np\u001b[38;5;241m.\u001b[39mexp(Z))\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m A\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "W1, b1, W2, b2 = gradient_descent(X_train, y_train, 1000, 0.5, 10, X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
